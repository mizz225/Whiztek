{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded!\n",
      "         user_id                                            Comment Sentiment  \\\n",
      "0  XYNN2Y4VCF3G  I bought 2200 at the ico, at 0.50$ per coin. H...  Positive   \n",
      "1  DR6XNZMT9KRH  Harmony one , algorand , Cardano, solana , vec...  Positive   \n",
      "2  9FCQGMYD4A42  Honestly, after reading this post and many of ...  Negative   \n",
      "3  QEZAEMV2WF9D  In bear market is where money is made. I Will ...  Positive   \n",
      "4  Z7J7W3XCP4XC  Funny how people think Bitcoin's risk is compa...  Negative   \n",
      "\n",
      "                                          Reddit URL  \n",
      "0  https://www.reddit.com/r/Avax/comments/uzggar/...  \n",
      "1  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "2  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "3  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "4  https://www.reddit.com/r/investing/comments/um...  \n",
      "Dataset Transformed!\n",
      "         user_id                                            Comment Sentiment  \\\n",
      "0  XYNN2Y4VCF3G  I bought 2200 at the ico, at 0.50$ per coin. H...  Positive   \n",
      "1  DR6XNZMT9KRH  Harmony one , algorand , Cardano, solana , vec...  Positive   \n",
      "2  9FCQGMYD4A42  Honestly, after reading this post and many of ...  Negative   \n",
      "3  QEZAEMV2WF9D  In bear market is where money is made. I Will ...  Positive   \n",
      "4  Z7J7W3XCP4XC  Funny how people think Bitcoin's risk is compa...  Negative   \n",
      "\n",
      "                                          Reddit URL  \\\n",
      "0  https://www.reddit.com/r/Avax/comments/uzggar/...   \n",
      "1  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "2  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "3  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
      "4  https://www.reddit.com/r/investing/comments/um...   \n",
      "\n",
      "                                       Clean_Comment  Label  \n",
      "0  I bought 2200 at the ico, at 0.50$ per coin. H...      1  \n",
      "1  Harmony one , algorand , Cardano, solana , vec...      1  \n",
      "2  Honestly, after reading this post and many of ...      0  \n",
      "3  In bear market is where money is made. I Will ...      1  \n",
      "4  Funny how people think Bitcoin's risk is compa...      0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"crypto_currency_sentiment_dataset.csv\")\n",
    "print(\"Dataset Loaded!\\n\", df.head())\n",
    "\n",
    "## SBert -- PCA -- XGB\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\\\S+|www\\\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.lower()  # Lowercase\n",
    "#     text = re.sub(r\"http\\\\S+|www\\\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#     text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])  # Lemmatize & remove stopwords\n",
    "#     return text\n",
    "\n",
    "df[\"Clean_Comment\"] = df[\"Comment\"].astype(str).apply(clean_text)\n",
    "\n",
    "df[\"Label\"] = df[\"Sentiment\"].map({\"Positive\": 1, \"Negative\": 0})\n",
    "\n",
    "print(\"Dataset Transformed!\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10_000, ngram_range=(1, 4), sublinear_tf=True, stop_words='english')\n",
    "X = tfidf.fit_transform(df[\"Clean_Comment\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "pca = PCA(n_components=512, svd_solver='auto')\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def comment_to_vec(comment, model, vector_size=300):\n",
    "    words = comment.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)  # Fallback for empty vectors\n",
    "\n",
    "df[\"Word2Vec_Feature\"] = df[\"Clean_Comment\"].apply(lambda x: comment_to_vec(x, w2v_model))\n",
    "\n",
    "X = np.vstack(df[\"Word2Vec_Feature\"].values)\n",
    "y = df[\"Label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\QCodes\\chatbot_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "e:\\QCodes\\chatbot_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n",
      "(562, 32)\n",
      "(562,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sbert_model = SentenceTransformer('./MiniLM_l6_v2')\n",
    "X = np.array([sbert_model.encode(text) for text in df[\"Clean_Comment\"]])\n",
    "y = df[\"Label\"].values\n",
    "\n",
    "pca = PCA(n_components=32) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(sbert_model.encode(\"How are you\").shape)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_and_evaluate(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"{model_name} Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{model_name} Performance on Test Set:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {acc_score:.4f}\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross-Validation Accuracy: 0.8238\n",
      "\n",
      "Logistic Regression Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88        26\n",
      "           1       0.88      0.94      0.91        31\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.90      0.89      0.89        57\n",
      "weighted avg       0.90      0.89      0.89        57\n",
      "\n",
      "Logistic Regression Accuracy: 0.8947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log_reg = LogisticRegression(max_iter=1000, C=1.0, class_weight='balanced')\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "train_and_evaluate(log_reg, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized XGBoost Cross-Validation Accuracy: 0.8000\n",
      "\n",
      "Optimized XGBoost Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        26\n",
      "           1       0.89      1.00      0.94        31\n",
      "\n",
      "    accuracy                           0.93        57\n",
      "   macro avg       0.94      0.92      0.93        57\n",
      "weighted avg       0.94      0.93      0.93        57\n",
      "\n",
      "Optimized XGBoost Accuracy: 0.9298\n",
      "\n",
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 300}\n",
      "Best Cross-Validation Accuracy: 0.7940734479196019\n"
     ]
    }
   ],
   "source": [
    "# xgb = XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth=6, random_state=42)\n",
    "# train_and_evaluate(xgb, \"XGBoost\")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "grid = GridSearchCV(xgb, params, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = grid.best_estimator_\n",
    "train_and_evaluate(best_xgb, \"Optimized XGBoost\")\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb, params, n_iter=30, cv=3, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "train_and_evaluate(best_xgb, \"Randomized Search Optimized XGBoost\")\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Cross-Validation Accuracy: 0.7921\n",
      "\n",
      "XGBoost Classifier Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88        26\n",
      "           1       0.88      0.94      0.91        31\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.90      0.89      0.89        57\n",
      "weighted avg       0.90      0.89      0.89        57\n",
      "\n",
      "XGBoost Classifier Accuracy: 0.8947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1, max_depth=8, n_estimators=300)\n",
    "train_and_evaluate(xgb, \"XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Cross-Validation Accuracy: 0.8000\n",
      "\n",
      "XGBoost Classifier Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        26\n",
      "           1       0.89      1.00      0.94        31\n",
      "\n",
      "    accuracy                           0.93        57\n",
      "   macro avg       0.94      0.92      0.93        57\n",
      "weighted avg       0.94      0.93      0.93        57\n",
      "\n",
      "XGBoost Classifier Accuracy: 0.9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.2, max_depth=6, n_estimators=300)\n",
    "xgb = train_and_evaluate(xgb, \"XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model_9298.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(xgb, \"xgboost_model_9298.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9378\n",
      "Recall: 0.9298\n",
      "F1 Score: 0.9290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Get predictions for the test set using the best XGBoost model\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, and F1-score explicitly\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'macro' or 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb.save_model(\"xgboost_model_9298.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(xgb, \"xgboost_model_9123.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "train_and_evaluate(rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9316\n",
      "Recall: 0.9298\n",
      "F1 Score: 0.9295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "loaded_xgb = XGBClassifier()\n",
    "loaded_xgb.load_model(\"xgboost_model_9298.json\")\n",
    "\n",
    "y_pred = loaded_xgb.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categ_env_TF_cpu  -- python 3.9.8\n",
    "\n",
    "# xgboost_model_9298.json -- XGBClassifier(learning_rate=0.2, max_depth=6, n_estimators=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
